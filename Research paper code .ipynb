{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first i have to connect with the GEE google earth engine by connecting to the account (getting the token ID)\n",
    "#!pip install earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94150e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "#shapefile= gpd.read_file(\"Data/Data-Mohi-help/Data/Phnompenh_diss_UTM.shp\")\n",
    "shapefile=gpd.read_file(\"Raster to vector/water_clipped_from_2016.shp\")\n",
    "#shapefile_fire= gpd.read_file(\"EMSR704_AOI01_DEL_PRODUCT_v2/EMSR704_AOI01_DEL_PRODUCT_observedEventL_v1.shp\")\n",
    "print (shapefile)\n",
    "shapefile.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model \n",
    "import ee\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize Earth Engine\n",
    "#ee.Initialize()\n",
    "\n",
    "# Load the training and validation tables\n",
    "train_table = ee.FeatureCollection('projects/ee-alidia1998201811/assets/1000_training_points')\n",
    "val_table = ee.FeatureCollection('projects/ee-alidia1998201811/assets/500_validationPoints')\n",
    "\n",
    "# Extract training features and labels\n",
    "train_features = np.array(train_table.aggregate_array('training17').getInfo())\n",
    "for prop in ['training18', 'training19', 'training20', 'training21', 'training22','training23']:\n",
    "    train_features = np.column_stack((train_features, np.array(train_table.aggregate_array(prop).getInfo())))\n",
    "train_labels = np.array(train_table.aggregate_array('training16').getInfo())\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "print('Model trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ee\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Load the training and validation tables\n",
    "train_table = ee.FeatureCollection('projects/ee-alidia1998201811/assets/1000_training_points')\n",
    "val_table = ee.FeatureCollection('projects/ee-alidia1998201811/assets/500_validationPoints')\n",
    "\n",
    "# Define training and validation properties per year\n",
    "years = list(range(2016, 2024))  # From 2016 to 2023\n",
    "train_properties = [f'training{year % 100}' for year in years]  # ['training16', 'training17', ..until 2023..]\n",
    "valid_properties = [f'{year}_valid' for year in years]  # ['2016_valid', '2017_valid', ..until 2023..]\n",
    "\n",
    "# Initialize lists to hold all training and validation data\n",
    "all_train_features = []\n",
    "all_train_labels = []\n",
    "all_val_features = []\n",
    "all_val_labels = []\n",
    "\n",
    "# Loop through each year for independent training and validation\n",
    "for train_prop, valid_prop, year in zip(train_properties, valid_properties, years):\n",
    "    print(f\"\\nProcessing Year: {year}\")\n",
    "\n",
    "    # Extract training features and labels for the current year\n",
    "    train_features = np.array(train_table.aggregate_array(train_prop).getInfo()).reshape(-1, 1).astype(float)\n",
    "    train_labels = np.array(train_table.aggregate_array('training16').getInfo())  \n",
    "\n",
    "    # Extract validation features and labels for the current year\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "\n",
    "    for feature in val_table.getInfo()['features']:\n",
    "        properties = feature['properties']\n",
    "        if valid_prop in properties:  # Ensure the validation property exists, otherwise it will never work\n",
    "            val_features.append([properties[valid_prop]])  \n",
    "            val_labels.append(properties['2016_valid'])  \n",
    "\n",
    "    val_features = np.array(val_features).astype(float)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    # Check if there's sufficient data\n",
    "    if len(train_features) == 0 or len(val_features) == 0:\n",
    "        print(f\"Skipping {year} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    # Append to the overall lists\n",
    "    all_train_features.append(train_features)\n",
    "    all_train_labels.append(train_labels)\n",
    "    all_val_features.append(val_features)\n",
    "    all_val_labels.append(val_labels)\n",
    "\n",
    "# Combine all training and validation data\n",
    "all_train_features = np.vstack(all_train_features)\n",
    "all_train_labels = np.concatenate(all_train_labels)\n",
    "all_val_features = np.vstack(all_val_features)\n",
    "all_val_labels = np.concatenate(all_val_labels)\n",
    "\n",
    "# Train the SVM model on the combined training data\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(all_train_features, all_train_labels)\n",
    "\n",
    "# Predict validation labels on the combined validation data\n",
    "val_predictions = svm_classifier.predict(all_val_features)\n",
    "\n",
    "# Evaluate overall model accuracy\n",
    "overall_accuracy = accuracy_score(all_val_labels, val_predictions)\n",
    "print(f'Overall Validation Accuracy: {overall_accuracy:.4f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_val_labels, val_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Water', 'Water'], yticklabels=['No Water', 'Water'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Overall Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a712bd",
   "metadata": {},
   "source": [
    "## SVM & sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Define Area of Interest (ROI)\n",
    "roi = ee.FeatureCollection('projects/ee-alidia1998201811/assets/5kmFishnet').geometry()\n",
    "\n",
    "# Cloud masking function for Sentinel-2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def maskS2clouds(image):\n",
    "    qa = image.select('QA60')\n",
    "    # Bits 10 and 11 are clouds and cirrus\n",
    "    cloudBitMask = 1 << 10\n",
    "    cirrusBitMask = 1 << 11\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\n",
    "        qa.bitwiseAnd(cirrusBitMask).eq(0)\n",
    "    )\n",
    "    return image.updateMask(mask).divide(10000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Sentinel-2 Data, apply cloud mask, and mosaic for full coverage\n",
    "sentinel2 = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .filterDate('2023-01-01', '2023-01-31') \\\n",
    "    .map(maskS2clouds) \\\n",
    "    #.select(['B3', 'B8'])\n",
    "    .select(['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B10', 'B11', 'B12'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "band_names = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B10', 'B11', 'B12']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mosaic to ensure full coverage over the ROI\n",
    "sentinel2_mosaic = sentinel2.mosaic().clip(roi)\n",
    "\n",
    "# Compute NDWI\n",
    "ndwi = sentinel2_mosaic.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "\n",
    "# Load Training Data\n",
    "trainingData = ee.FeatureCollection('projects/ee-alidia1998201811/assets/1000_training_points')\n",
    "\n",
    "# Function to aggregate water classes from training properties\n",
    "def merge_classes(feature):\n",
    "    water_label = ee.Number(0)\n",
    "    for i in range(16, 24):  # Aggregating water classes\n",
    "        water_label = water_label.add(feature.getNumber(f'training{i}'))\n",
    "    return feature.set('water_class', ee.Algorithms.If(water_label.gt(0), 1, 0))\n",
    "\n",
    "# Apply function to training data\n",
    "trainingData = trainingData.map(merge_classes)\n",
    "\n",
    "# Extract NDWI values for training\n",
    "training = ndwi.sampleRegions(\n",
    "    collection=trainingData,\n",
    "    properties=['water_class'],\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "# Train SVM Classifier\n",
    "svmClassifier = ee.Classifier.libsvm(kernelType='RBF', gamma=0.5, cost=10) \\\n",
    "    .train(features=training, classProperty='water_class', inputProperties=['band_names'])\n",
    "\n",
    "# Apply SVM Model to classify Sentinel-2 data\n",
    "classified = ndwi.classify(svmClassifier)\n",
    "\n",
    "# Load Validation Data\n",
    "validationData = ee.FeatureCollection('projects/ee-alidia1998201811/assets/500_validationPoints')\n",
    "\n",
    "# Sample the classified image using the validation data\n",
    "years = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]  # Years of interest\n",
    "validation_samples = classified.sampleRegions(\n",
    "    collection=validationData,\n",
    "    properties=[f\"{year}_valid\" for year in years],\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "# Check extracted validation samples\n",
    "validation_samples_info = validation_samples.getInfo()\n",
    "print(\"Sample of validation samples:\", validation_samples_info)\n",
    "\n",
    "# Select the correct validation column (e.g., latest year 2023)\n",
    "confusionMatrix = validation_samples.errorMatrix(\n",
    "    actual='2023_valid',  # Adjust based on the chosen validation year\n",
    "    predicted='classification'\n",
    ")\n",
    "\n",
    "# Extract confusion matrix values as a 2D array\n",
    "confusion_values = np.array(confusionMatrix.getInfo())\n",
    "\n",
    "# Check if confusion matrix is correctly structured\n",
    "print(\"Confusion matrix values:\", confusion_values)\n",
    "if confusion_values.shape != (2, 2):\n",
    "    print(\"Warning: Confusion matrix is not 2x2, check your classes!\")\n",
    "\n",
    "# Define labels\n",
    "labels = ['Non-water (0)', 'Water (1)']\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_values, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Sentinel-2 Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: choose a specific month (e.g., March 2023)\n",
    "start_date = '2019-01-01'\n",
    "end_date   = '2019-01-31'\n",
    "\n",
    "# Filter Sentinel-2 collection for that month\n",
    "monthly_collection = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .map(maskS2clouds) \\\n",
    "    .select(['B3', 'B8', 'B4', 'B2'])\n",
    "\n",
    "# Mosaic all images for full coverage\n",
    "specific_image = monthly_collection.mosaic().clip(roi)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Compute NDWI for the mosaic\n",
    "# -------------------------------\n",
    "ndwi_single = specific_image.normalizedDifference(['B3', 'B8']).rename('NDWI').clip(roi)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Apply classifier\n",
    "# -------------------------------\n",
    "classified_single = ndwi_single.classify(svmClassifier)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Visualize\n",
    "# -------------------------------\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(roi, 10)\n",
    "\n",
    "Map.addLayer(specific_image, \n",
    "             {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3}, \n",
    "             'RGB (Monthly Mosaic)')\n",
    "Map.addLayer(ndwi_single, \n",
    "             {'min': -1, 'max': 1, 'palette': ['brown', 'blue']}, \n",
    "             'NDWI (Monthly)')\n",
    "Map.addLayer(classified_single, \n",
    "             {'min': 0, 'max': 1, 'palette': ['gray', 'blue']}, \n",
    "             'Classified (Monthly)')\n",
    "\n",
    "Map.addLayerControl()\n",
    "Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efc1fa",
   "metadata": {},
   "source": [
    "## SVM & Sentinel-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94aeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Define Area of Interest (ROI)\n",
    "roi = ee.FeatureCollection('projects/ee-alidia1998201811/assets/5kmFishnet').geometry()\n",
    "\n",
    "# Load Sentinel-1 Data\n",
    "sentinel1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .filterDate('2023-01-01', '2023-12-31') \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n",
    "    .filter(ee.Filter.eq('resolution_meters', 10)) \\\n",
    "    .select(['VV', 'VH'])\n",
    "\n",
    "# Compute median composite\n",
    "sentinel1_median = sentinel1.median().clip(roi)\n",
    "\n",
    "# Load Training Data\n",
    "trainingData = ee.FeatureCollection('projects/ee-alidia1998201811/assets/1000_training_points')\n",
    "\n",
    "# Function to safely get properties\n",
    "def get_property_or_default(feature, property_name):\n",
    "    return ee.Algorithms.If(feature.get(property_name), feature.get(property_name), 0)\n",
    "\n",
    "# Function to merge water training columns\n",
    "def merge_classes(feature):\n",
    "    water_label = ee.Number(0)\n",
    "    #  water class is determined from columns 2016 to 2023\n",
    "    for i in range(16, 24): \n",
    "        water_label = water_label.add(get_property_or_default(feature, f'training{i}'))\n",
    "    return feature.set('water_class', ee.Algorithms.If(water_label.gt(0), 1, 0))\n",
    "\n",
    "# Apply function to training data\n",
    "trainingData = trainingData.map(merge_classes)\n",
    "\n",
    "# Extract Sentinel-1 values for training\n",
    "training = sentinel1_median.sampleRegions(\n",
    "    collection=trainingData,\n",
    "    properties=['water_class'],\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "# Train SVM Classifier\n",
    "svmClassifier = ee.Classifier.libsvm(\n",
    "    kernelType='RBF',\n",
    "    gamma=0.5,\n",
    "    cost=10\n",
    ").train(features=training, classProperty='water_class', inputProperties=['VV', 'VH'])\n",
    "\n",
    "# Apply SVM Model\n",
    "classified = sentinel1_median.classify(svmClassifier)\n",
    "\n",
    "# Load Validation Data\n",
    "validationData = ee.FeatureCollection('projects/ee-alidia1998201811/assets/500_validationPoints')\n",
    "\n",
    "# Sample the classified image using the validation data\n",
    "# Specify the years of interest for validation\n",
    "years = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "validation_samples = classified.sampleRegions(\n",
    "    collection=validationData,\n",
    "    properties=[f\"{year}_valid\" for year in years],\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "# Check the structure and class distribution in validation samples\n",
    "validation_samples_info = validation_samples.getInfo()\n",
    "print(\"Sample of validation samples:\", validation_samples_info)\n",
    "\n",
    "# Assuming we want to classify based on one of the columns, say 2023_valid\n",
    "# we can adjust this based on the actual label column you want to compare against.\n",
    "# Let's assume we are comparing against the last valid year column, `2023_valid`.\n",
    "confusionMatrix = validation_samples.errorMatrix(\n",
    "    actual='2023_valid',  # Change as necessary if you're using a different year\n",
    "    predicted='classification'\n",
    ")\n",
    "\n",
    "# Extract the confusion matrix values as a 2D array\n",
    "confusion_values = np.array(confusionMatrix.getInfo())\n",
    "\n",
    "# Check the structure of confusion_values\n",
    "print(\"Confusion matrix values:\", confusion_values)\n",
    "\n",
    "# Ensure confusion values are properly shaped (should be 2x2)\n",
    "if confusion_values.shape != (2, 2):\n",
    "    print(\"Warning: Confusion matrix is not 2x2, check your classes!\")\n",
    "\n",
    "# Define labels for the classes\n",
    "labels = ['Non-water (0)', 'Water (1)']  # Adjust based on your classes\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_values, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = '2016-01-01'\n",
    "end_date = '2016-01-30'\n",
    "\n",
    "sentinel1_month = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n",
    "    .filter(ee.Filter.eq('resolution_meters', 10)) \\\n",
    "    .select(['VV', 'VH']) \\\n",
    "    .median() \\\n",
    "    .clip(roi)\n",
    "\n",
    "classified_month = sentinel1_month.classify(svmClassifier)\n",
    "\n",
    "Map = geemap.Map()\n",
    "Map.addLayer(sentinel1_month.select('VV'), {'min': -25, 'max': 5}, 'Sentinel-1 VV (monthly)')\n",
    "Map.addLayer(classified_month, vis_params_class, 'Classification (monthly)')\n",
    "Map.centerObject(roi, 9)\n",
    "Map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ca2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e88c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
